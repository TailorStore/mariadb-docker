#!/bin/bash

# get_database_dump - Interactive S3 database backup downloader and importer
# Downloads database dumps from S3 and imports them into MariaDB

set -euo pipefail

# Configuration
S3_BUCKET="tailorstore-db"
S3_REGION="eu-west-1"
TEMP_DIR="/tmp"
COMPOSE_FILE="docker-compose.yml"

# Database files will be loaded dynamically from S3
declare -a DB_FILES=()

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

check_requirements() {
    log_info "Checking requirements..."
    
    # Check if AWS CLI is installed
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI is not installed. Please install it first."
        exit 1
    fi
    
    # Check if pv is installed
    if ! command -v pv &> /dev/null; then
        log_error "pv (Pipe Viewer) is not installed. Please install it first."
        exit 1
    fi
    
    # Check environment variables
    if [[ -z "${AWS_S3_ACCESS_ID:-}" ]]; then
        log_error "AWS_S3_ACCESS_ID environment variable is not set."
        exit 1
    fi
    
    if [[ -z "${AWS_S3_SECRET_ACCESS_KEY:-}" ]]; then
        log_error "AWS_S3_SECRET_ACCESS_KEY environment variable is not set."
        exit 1
    fi
    
    # Check if MariaDB is accessible (we're running inside the container)
    if ! mariadb --version &> /dev/null; then
        log_error "MariaDB is not available."
        exit 1
    fi
    
    log_success "All requirements satisfied."
}

configure_aws() {
    log_info "Configuring AWS credentials..."
    export AWS_ACCESS_KEY_ID="$AWS_S3_ACCESS_ID"
    export AWS_SECRET_ACCESS_KEY="$AWS_S3_SECRET_ACCESS_KEY"
    export AWS_DEFAULT_REGION="$S3_REGION"
}

load_database_files() {
    log_info "Fetching available database files from S3..."
    
    # Get detailed list of .sql.gz files from S3 bucket with size and date info
    local s3_output
    s3_output=$(aws s3 ls "s3://$S3_BUCKET/" --region "$S3_REGION" --human-readable | grep '\.sql\.gz$')
    
    if [[ -z "$s3_output" ]]; then
        log_error "No .sql.gz files found in S3 bucket: s3://$S3_BUCKET/"
        log_info "Available files in bucket:"
        aws s3 ls "s3://$S3_BUCKET/" --region "$S3_REGION" || log_error "Failed to list bucket contents"
        exit 1
    fi
    
    # Clear the arrays and populate with S3 files
    DB_FILES=()
    declare -g -A DB_FILE_INFO
    
    while IFS= read -r line; do
        if [[ -n "$line" ]]; then
            # Parse the line: date time size unit filename
            local parts=($line)
            local date_part="${parts[0]}"
            local time_part="${parts[1]}"
            local size_part="${parts[2]}"
            local unit_part="${parts[3]}"
            local filename="${parts[4]}"
            
            if [[ -n "$filename" && "$filename" == *.sql.gz ]]; then
                DB_FILES+=("$filename")
                DB_FILE_INFO["$filename"]="$size_part $unit_part ($date_part $time_part)"
            fi
        fi
    done <<< "$s3_output"
    
    # Sort the files array
    IFS=$'\n' DB_FILES=($(sort <<< "${DB_FILES[*]}"))
    unset IFS
    
    log_success "Found ${#DB_FILES[@]} database file(s) in S3 bucket"
}

show_menu() {
    echo
    echo "=============================================="
    echo "    Database Backup Download & Import"
    echo "=============================================="
    echo
    echo "Available database files:"
    echo
    
    for i in "${!DB_FILES[@]}"; do
        local filename="${DB_FILES[$i]}"
        local info="${DB_FILE_INFO[$filename]:-}"
        if [[ -n "$info" ]]; then
            printf "%d) %-25s %s\n" $((i+1)) "$filename" "$info"
        else
            printf "%d) %s\n" $((i+1)) "$filename"
        fi
    done
    
    echo
    echo "0) Exit"
    echo
}

get_user_choice() {
    local choice
    while true; do
        read -p "Select a database file to download and import (0-${#DB_FILES[@]}): " choice
        
        if [[ "$choice" == "0" ]]; then
            log_info "Exiting..."
            exit 0
        elif [[ "$choice" =~ ^[0-9]+$ ]] && [[ "$choice" -le "${#DB_FILES[@]}" ]] && [[ "$choice" -gt 0 ]]; then
            SELECTED_FILE="${DB_FILES[$((choice-1))]}"
            log_info "Selected: $SELECTED_FILE"
            break
        else
            log_error "Invalid choice. Please select a number between 0 and ${#DB_FILES[@]}."
        fi
    done
}

download_file() {
    local file="$1"
    local local_path="$TEMP_DIR/$file"
    local s3_url="s3://$S3_BUCKET/$file"
    
    log_info "Downloading $file from S3..."
    log_info "Source: $s3_url"
    log_info "Destination: $local_path"
    
    # Get expected file size from S3 for pv progress monitoring
    local expected_size
    expected_size=$(aws s3 ls "s3://$S3_BUCKET/$file" --region "$S3_REGION" | awk '{print $3}')
    
    if [[ -n "$expected_size" && "$expected_size" =~ ^[0-9]+$ ]]; then
        # Format size for display (simple human-readable conversion)
        local size_display
        if [[ $expected_size -gt 1073741824 ]]; then
            size_display="$(( expected_size / 1073741824 ))GB"
        elif [[ $expected_size -gt 1048576 ]]; then
            size_display="$(( expected_size / 1048576 ))MB"
        elif [[ $expected_size -gt 1024 ]]; then
            size_display="$(( expected_size / 1024 ))KB"
        else
            size_display="${expected_size}B"
        fi
        
        log_info "Downloading with progress (Expected size: $size_display)..."
        # Use AWS CLI with pv for progress monitoring
        if aws s3 cp "$s3_url" - --region "$S3_REGION" --quiet | pv -s "$expected_size" > "$local_path"; then
            log_success "Download completed: $local_path"
        else
            log_error "Failed to download $file from S3"
            exit 1
        fi
    else
        log_info "Downloading with progress (size unknown)..."
        # Use pv without size information for basic progress indication
        if aws s3 cp "$s3_url" - --region "$S3_REGION" --quiet | pv > "$local_path"; then
            log_success "Download completed: $local_path"
        else
            log_error "Failed to download $file from S3"
            exit 1
        fi
    fi
    
    # Show actual downloaded file size
    local file_size
    if [[ -f "$local_path" ]]; then
        # Use ls to get file size (more portable than stat)
        file_size=$(ls -l "$local_path" | awk '{print $5}' 2>/dev/null || echo "unknown")
        if [[ "$file_size" != "unknown" && "$file_size" =~ ^[0-9]+$ ]]; then
            if [[ $file_size -gt 1073741824 ]]; then
                file_size="$(( file_size / 1073741824 ))GB"
            elif [[ $file_size -gt 1048576 ]]; then
                file_size="$(( file_size / 1048576 ))MB"
            elif [[ $file_size -gt 1024 ]]; then
                file_size="$(( file_size / 1024 ))KB"
            else
                file_size="${file_size}B"
            fi
        fi
        log_info "Downloaded file size: $file_size"
    else
        log_error "Download file not found at: $local_path"
        exit 1
    fi
}

optimize_for_import() {
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Applying import optimizations..."
    
    # Apply production-safe import optimizations
    mariadb -u "$db_user" -p"$db_password" -e "
        SET GLOBAL innodb_flush_log_at_trx_commit = 0;
        SET GLOBAL sync_binlog = 0;
        SET GLOBAL innodb_io_capacity = 2000;
        SET GLOBAL innodb_io_capacity_max = 4000;
        SET GLOBAL key_buffer_size = 128*1024*1024;
        SET GLOBAL max_allowed_packet = 1073741824;
        SET GLOBAL query_cache_size = 0;
        SELECT 'Import optimizations applied' AS Status;
    " 2>/dev/null || log_warning "Some optimizations could not be applied"
}

restore_production_settings() {
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Restoring production-safe settings..."
    
    # Restore production-safe settings
    mariadb -u "$db_user" -p"$db_password" -e "
        SET GLOBAL innodb_flush_log_at_trx_commit = 2;
        SET GLOBAL sync_binlog = 1;
        SET GLOBAL innodb_io_capacity = 200;
        SET GLOBAL innodb_io_capacity_max = 400;
        SET GLOBAL key_buffer_size = 32*1024*1024;
        SET GLOBAL max_allowed_packet = 64*1024*1024;
        SET GLOBAL query_cache_size = 128*1024*1024;
        SELECT 'Production settings restored' AS Status;
    " 2>/dev/null || log_warning "Some settings could not be restored"
}

import_database() {
    local file_path="$1"
    local file_name
    # Use bash parameter expansion instead of basename to avoid argument length issues
    file_name="${file_path##*/}"
    
    log_info "Starting database import..."
    log_info "File: $file_name"
    
    # Get database name from environment or use default
    local db_name="${MYSQL_DATABASE:-myapp}"
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Importing into database: $db_name"
    
    # Apply import optimizations before starting
    optimize_for_import
    
    # Import with progress and optimizations
    log_info "Decompressing and importing database (this may take a while)..."
    
    # Use optimized import with disabled checks for maximum speed
    if {
        echo "SET foreign_key_checks = 0;";
        echo "SET unique_checks = 0;";
        echo "SET autocommit = 0;";
        pv "$file_path" | gunzip;
        echo "COMMIT;";
        echo "SET foreign_key_checks = 1;";
        echo "SET unique_checks = 1;";
        echo "SET autocommit = 1;";
    } | mariadb -u "$db_user" -p"$db_password" "$db_name"; then
        log_success "Database import completed successfully!"
        
        # Restore production settings after successful import
        restore_production_settings
    else
        log_error "Database import failed!"
        # Restore production settings even on failure
        restore_production_settings
        exit 1
    fi
}

cleanup_file() {
    local file_path="$1"
    
    log_info "Cleaning up downloaded file..."
    
    if rm -f "$file_path"; then
        log_success "Temporary file deleted: $file_path"
    else
        log_warning "Failed to delete temporary file: $file_path"
    fi
}

confirm_import() {
    local file="$1"
    
    echo
    log_warning "⚠️  WARNING: This will import data into your MariaDB database!"
    log_warning "⚠️  This operation may overwrite existing data!"
    echo
    echo "Selected file: $file"
    echo "Database: ${MYSQL_DATABASE:-myapp}"
    echo
    
    read -p "Are you sure you want to proceed? (yes/no): " confirmation
    
    case "$confirmation" in
        [Yy][Ee][Ss]|[Yy])
            log_info "Proceeding with import..."
            return 0
            ;;
        *)
            log_info "Import cancelled by user."
            exit 0
            ;;
    esac
}

main() {
    echo "🗄️  Database Backup Downloader & Importer"
    echo "========================================="
    
    # Check all requirements
    check_requirements
    
    # Configure AWS
    configure_aws
    
    # Load database files from S3
    load_database_files
    
    # Show menu and get user choice
    show_menu
    get_user_choice
    
    # Confirm the import operation
    confirm_import "$SELECTED_FILE"
    
    # Download the selected file
    local downloaded_file="$TEMP_DIR/$SELECTED_FILE"
    
    # Run download function
    download_file "$SELECTED_FILE"
    
    # Verify the file was downloaded
    if [[ ! -f "$downloaded_file" ]]; then
        log_error "Downloaded file not found: $downloaded_file"
        exit 1
    fi
    
    # Import the database
    import_database "$downloaded_file"
    
    # Clean up
    cleanup_file "$downloaded_file"
    
    echo
    log_success "✅ Database import process completed successfully!"
    echo
}

# Handle script interruption
trap 'log_error "Script interrupted. Cleaning up..."; exit 1' INT TERM

# Run main function
main "$@"
