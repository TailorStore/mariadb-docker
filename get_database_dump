#!/bin/bash

# get_database_dump - Interactive S3 database backup downloader and importer
# Downloads database dumps from S3 and imports them into MariaDB

set -uo pipefail

# Configuration
S3_BUCKET="tailorstore-db"
S3_REGION="eu-west-1"
TEMP_DIR="/tmp"
COMPOSE_FILE="docker-compose.yml"

# Command line flags
PURGE_BINLOGS=false

# Database files will be loaded dynamically from S3
declare -a DB_FILES=()

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
# Function to check if pv is working properly
check_pv_working() {
    # Test if pv displays progress properly in current environment
    echo "test" | pv -f -q >/dev/null 2>&1
    return $?
}

# Function to show download progress with fallback
show_download_progress() {
    local file="$1"
    local expected_size="$2"
    
    if check_pv_working; then
        if [[ -n "$expected_size" && "$expected_size" =~ ^[0-9]+$ ]]; then
            pv -f -s "$expected_size" -N "Downloading $file"
        else
            pv -f -N "Downloading $file"
        fi
    else
        log_info "Progress monitoring unavailable, downloading in background..."
        cat
    fi
}

# Function to show import progress with fallback
show_import_progress() {
    local file_path="$1"
    
    if check_pv_working; then
        pv -f -N "Importing $(basename "$file_path")" "$file_path"
    else
        log_info "Progress monitoring unavailable, importing in background..."
        cat "$file_path"
    fi
}

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" >&2
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

show_help() {
    cat << EOF
Database Backup Downloader & Importer

USAGE:
    get_database_dump [OPTIONS]

OPTIONS:
    --purge-binlogs    Delete all existing binary logs before import
                       (Recommended for development/testing environments)
    --help, -h         Show this help message

DESCRIPTION:
    Interactive script to download database backups from S3 and import them
    into MariaDB. The script will:
    
    1. List available .sql.gz files from the S3 bucket
    2. Allow you to select which file to import
    3. Download the selected file with progress monitoring
    4. Apply import optimizations (including disabling binary logging)
    5. Import the database with progress indication
    6. Restore production-safe settings after import
    
BINARY LOG CLEANUP:
    Use --purge-binlogs to delete existing binary logs before import.
    This is recommended for development/testing but use with caution in
    production environments where binary logs are needed for replication
    or point-in-time recovery.

EXAMPLES:
    get_database_dump                    # Standard import (keep existing binary logs)
    get_database_dump --purge-binlogs    # Import with binary log cleanup

NOTES:
    - Binary logging is temporarily disabled during import for performance
    - Import optimizations are applied and restored automatically
    - The script must be run inside the MariaDB container
    - AWS credentials must be configured via environment variables
EOF
}

parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --purge-binlogs)
                PURGE_BINLOGS=true
                shift
                ;;
            --help|-h)
                # Help is handled in main, but keep this for completeness
                shift
                ;;
            *)
                log_error "Unknown option: $1"
                echo "Use --help for usage information"
                exit 1
                ;;
        esac
    done
}

check_requirements() {
    log_info "Checking requirements..."
    
    # Check if AWS CLI is installed
    if ! command -v aws &> /dev/null; then
        log_error "AWS CLI is not installed. Please install it first."
        return 1
    fi
    
    # Check if pv is installed
    if ! command -v pv &> /dev/null; then
        log_error "pv (Pipe Viewer) is not installed. Please install it first."
        return 1
    fi
    
    # Check environment variables
    if [[ -z "${AWS_S3_ACCESS_ID:-}" ]]; then
        log_error "AWS_S3_ACCESS_ID environment variable is not set."
        return 1
    fi
    
    if [[ -z "${AWS_S3_SECRET_ACCESS_KEY:-}" ]]; then
        log_error "AWS_S3_SECRET_ACCESS_KEY environment variable is not set."
        return 1
    fi
    
    # Check if MariaDB is accessible (we're running inside the container)
    if ! mariadb --version &> /dev/null; then
        log_error "MariaDB is not available."
        return 1
    fi
    
    log_success "All requirements satisfied."
    return 0
}

configure_aws() {
    log_info "Configuring AWS credentials..."
    export AWS_ACCESS_KEY_ID="$AWS_S3_ACCESS_ID"
    export AWS_SECRET_ACCESS_KEY="$AWS_S3_SECRET_ACCESS_KEY"
    export AWS_DEFAULT_REGION="$S3_REGION"
}

load_database_files() {
    log_info "Fetching available database files from S3..."
    
    # Get detailed list of .sql.gz files from S3 bucket with size and date info
    local s3_output
    s3_output=$(aws s3 ls "s3://$S3_BUCKET/" --region "$S3_REGION" --human-readable | grep '\.sql\.gz$')
    
    if [[ -z "$s3_output" ]]; then
        log_error "No .sql.gz files found in S3 bucket: s3://$S3_BUCKET/"
        log_info "Available files in bucket:"
        aws s3 ls "s3://$S3_BUCKET/" --region "$S3_REGION" || log_error "Failed to list bucket contents"
        return 1
    fi
    
    # Clear the arrays and populate with S3 files
    DB_FILES=()
    declare -g -A DB_FILE_INFO
    
    while IFS= read -r line; do
        if [[ -n "$line" ]]; then
            # Parse the line: date time size unit filename
            local parts=($line)
            local date_part="${parts[0]}"
            local time_part="${parts[1]}"
            local size_part="${parts[2]}"
            local unit_part="${parts[3]}"
            local filename="${parts[4]}"
            
            if [[ -n "$filename" && "$filename" == *.sql.gz ]]; then
                DB_FILES+=("$filename")
                DB_FILE_INFO["$filename"]="$size_part $unit_part ($date_part $time_part)"
            fi
        fi
    done <<< "$s3_output"
    
    # Sort the files array
    IFS=$'\n' DB_FILES=($(sort <<< "${DB_FILES[*]}"))
    unset IFS
    
    log_success "Found ${#DB_FILES[@]} database file(s) in S3 bucket"
}

show_menu() {
    echo
    echo "=============================================="
    echo "    Database Backup Download & Import"
    echo "=============================================="
    echo
    echo "Available database files:"
    echo
    
    for i in "${!DB_FILES[@]}"; do
        local filename="${DB_FILES[$i]}"
        local info="${DB_FILE_INFO[$filename]:-}"
        if [[ -n "$info" ]]; then
            printf "%d) %-25s %s\n" $((i+1)) "$filename" "$info"
        else
            printf "%d) %s\n" $((i+1)) "$filename"
        fi
    done
    
    echo
    echo "0) Exit"
    echo
}

get_user_choice() {
    local choice
    while true; do
        read -p "Select a database file to download and import (0-${#DB_FILES[@]}): " choice
        
        if [[ "$choice" == "0" ]]; then
            log_info "Exiting..."
            return 1
        elif [[ "$choice" =~ ^[0-9]+$ ]] && [[ "$choice" -le "${#DB_FILES[@]}" ]] && [[ "$choice" -gt 0 ]]; then
            SELECTED_FILE="${DB_FILES[$((choice-1))]}"
            log_info "Selected: $SELECTED_FILE"
            break
        else
            log_error "Invalid choice. Please select a number between 0 and ${#DB_FILES[@]}."
        fi
    done
}

download_file() {
    local file="$1"
    local local_path="$TEMP_DIR/$file"
    local s3_url="s3://$S3_BUCKET/$file"
    
    log_info "Downloading $file from S3..."
    log_info "Source: $s3_url"
    log_info "Destination: $local_path"
    
    # Get expected file size from S3 for pv progress monitoring
    local expected_size
    expected_size=$(aws s3 ls "s3://$S3_BUCKET/$file" --region "$S3_REGION" | awk '{print $3}')
    
    if [[ -n "$expected_size" && "$expected_size" =~ ^[0-9]+$ ]]; then
        # Format size for display (simple human-readable conversion)
        local size_display
        if [[ $expected_size -gt 1073741824 ]]; then
            size_display="$(( expected_size / 1073741824 ))GB"
        elif [[ $expected_size -gt 1048576 ]]; then
            size_display="$(( expected_size / 1048576 ))MB"
        elif [[ $expected_size -gt 1024 ]]; then
            size_display="$(( expected_size / 1024 ))KB"
        else
            size_display="${expected_size}B"
        fi
        
        log_info "Downloading with progress (Expected size: $size_display)..."
        # Use AWS CLI with progress monitoring
        if aws s3 cp "$s3_url" - --region "$S3_REGION" --quiet | show_download_progress "$file" "$expected_size" > "$local_path"; then
            log_success "Download completed: $local_path"
        else
            log_error "Failed to download $file from S3"
            return 1
        fi
    else
        log_info "Downloading with progress (size unknown)..."
        # Use progress monitoring without size information
        if aws s3 cp "$s3_url" - --region "$S3_REGION" --quiet | show_download_progress "$file" "" > "$local_path"; then
            log_success "Download completed: $local_path"
        else
            log_error "Failed to download $file from S3"
            return 1
        fi
    fi
    
    # Show actual downloaded file size
    local file_size
    if [[ -f "$local_path" ]]; then
        # Use ls to get file size (more portable than stat)
        file_size=$(ls -l "$local_path" | awk '{print $5}' 2>/dev/null || echo "unknown")
        if [[ "$file_size" != "unknown" && "$file_size" =~ ^[0-9]+$ ]]; then
            if [[ $file_size -gt 1073741824 ]]; then
                file_size="$(( file_size / 1073741824 ))GB"
            elif [[ $file_size -gt 1048576 ]]; then
                file_size="$(( file_size / 1048576 ))MB"
            elif [[ $file_size -gt 1024 ]]; then
                file_size="$(( file_size / 1024 ))KB"
            else
                file_size="${file_size}B"
            fi
        fi
        log_info "Downloaded file size: $file_size"
    else
        log_error "Download file not found at: $local_path"
        return 1
    fi
}

cleanup_binary_logs() {
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    if [[ "$PURGE_BINLOGS" != "true" ]]; then
        log_info "Skipping binary log cleanup (use --purge-binlogs to enable)"
        return 0
    fi
    
    log_info "Cleaning up old binary logs before import..."
    
    # Check if binary logging is enabled
    local binlog_enabled
    binlog_enabled=$(mariadb -u "$db_user" -p"$db_password" -se "SELECT @@log_bin;" 2>/dev/null)
    
    if [[ "$binlog_enabled" == "1" ]]; then
        # Get current binary log file count and size
        local log_count
        local log_size
        log_count=$(mariadb -u "$db_user" -p"$db_password" -se "SHOW BINARY LOGS;" 2>/dev/null | wc -l)
        
        if [[ $log_count -gt 0 ]]; then
            log_info "Found $log_count binary log files"
            
            # Calculate total size of binary logs using SHOW BINARY LOGS
            log_size=$(mariadb -u "$db_user" -p"$db_password" -se "SHOW BINARY LOGS;" 2>/dev/null | awk '{sum += \$2} END {printf "%.2f", sum/1024/1024}')
            
            if [[ -n "$log_size" && "$log_size" != "0.00" ]]; then
                log_info "Total binary log size: ${log_size}MB"
            fi
            
            # Purge all binary logs (keeps only the current active log)
            if mariadb -u "$db_user" -p"$db_password" -e "RESET MASTER;" 2>/dev/null; then
                log_success "Binary logs cleaned up successfully"
            else
                log_warning "Failed to clean up binary logs (continuing anyway)"
            fi
        else
            log_info "No binary logs found to clean up"
        fi
    else
        log_info "Binary logging is disabled, no cleanup needed"
    fi
}

optimize_for_import() {
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Applying import optimizations..."
    
    # Apply production-safe import optimizations
    mariadb -u "$db_user" -p"$db_password" -e "
        -- Disable durability and logging for speed
        SET GLOBAL innodb_flush_log_at_trx_commit = 0;
        SET GLOBAL sync_binlog = 0;
        SET sql_log_bin = 0;
        
        -- Disable constraint and validation checks
        SET foreign_key_checks = 0;
        SET unique_checks = 0;
        SET autocommit = 0;
        
        -- Optimize InnoDB for bulk loading
        SET GLOBAL innodb_io_capacity = 2000;
        SET GLOBAL innodb_io_capacity_max = 4000;
        SET GLOBAL innodb_buffer_pool_dump_at_shutdown = 0;
        SET GLOBAL innodb_doublewrite = 0;
        SET GLOBAL innodb_change_buffering = 'all';
        SET GLOBAL innodb_stats_on_metadata = 0;
        
        -- Increase buffer sizes
        SET GLOBAL key_buffer_size = 128*1024*1024;
        SET GLOBAL max_allowed_packet = 1073741824;
        SET GLOBAL sort_buffer_size = 32*1024*1024;
        SET GLOBAL bulk_insert_buffer_size = 256*1024*1024;
        
        -- Disable query cache and other overhead
        SET GLOBAL query_cache_size = 0;
        SET GLOBAL general_log = 0;
        SET GLOBAL slow_query_log = 0;
        
        SELECT 'Enhanced import optimizations applied (indexing/constraints optimized)' AS Status;
    " 2>/dev/null || log_warning "Some optimizations could not be applied"
}

restore_production_settings() {
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Restoring production-safe settings..."
    
    # Restore production-safe settings
    mariadb -u "$db_user" -p"$db_password" -e "
        -- Restore durability and logging
        SET GLOBAL innodb_flush_log_at_trx_commit = 2;
        SET GLOBAL sync_binlog = 1;
        SET sql_log_bin = 1;
        
        -- Re-enable constraint and validation checks
        SET foreign_key_checks = 1;
        SET unique_checks = 1;
        SET autocommit = 1;
        
        -- Restore InnoDB production settings
        SET GLOBAL innodb_io_capacity = 200;
        SET GLOBAL innodb_io_capacity_max = 400;
        SET GLOBAL innodb_buffer_pool_dump_at_shutdown = 1;
        SET GLOBAL innodb_doublewrite = 1;
        SET GLOBAL innodb_change_buffering = 'all';
        SET GLOBAL innodb_stats_on_metadata = 1;
        
        -- Restore buffer sizes to production values
        SET GLOBAL key_buffer_size = 32*1024*1024;
        SET GLOBAL max_allowed_packet = 64*1024*1024;
        SET GLOBAL sort_buffer_size = 4*1024*1024;
        SET GLOBAL bulk_insert_buffer_size = 8*1024*1024;
        
        -- Re-enable query cache and logging (if desired)
        SET GLOBAL query_cache_size = 128*1024*1024;
        SET GLOBAL general_log = 0;  -- Keep disabled for performance
        SET GLOBAL slow_query_log = 1;
        
        SELECT 'Production settings restored (all optimizations reverted)' AS Status;
    " 2>/dev/null || log_warning "Some settings could not be restored"
}

import_database() {
    local file_path="$1"
    local file_name
    # Use bash parameter expansion instead of basename to avoid argument length issues
    file_name="${file_path##*/}"
    
    log_info "Starting database import..."
    log_info "File: $file_name"
    
    # Get database name from environment or use default
    local db_name="${MYSQL_DATABASE:-myapp}"
    local db_user="root"
    local db_password="${MYSQL_ROOT_PASSWORD:-rootpassword}"
    
    log_info "Importing into database: $db_name"
    
    # Clean up old binary logs to free space and avoid confusion
    cleanup_binary_logs
    
    # Apply import optimizations before starting
    optimize_for_import
    
    # Import with progress and optimizations
    log_info "Decompressing and importing database (this may take a while)..."
    
    # Use optimized import with comprehensive speed optimizations
    if {
        echo "-- Session-level import optimizations";
        echo "SET SESSION sql_log_bin = 0;";
        echo "SET SESSION foreign_key_checks = 0;";
        echo "SET SESSION unique_checks = 0;";
        echo "SET SESSION autocommit = 0;";
        echo "SET SESSION sql_mode = '';";
        echo "SET SESSION innodb_lock_wait_timeout = 300;";
        echo "";
        show_import_progress "$file_path" | gunzip;
        echo "";
        echo "-- Commit and restore session settings";
        echo "COMMIT;";
        echo "SET SESSION autocommit = 1;";
        echo "SET SESSION foreign_key_checks = 1;";
        echo "SET SESSION unique_checks = 1;";
        echo "SET SESSION sql_log_bin = 1;";
        echo "SET SESSION sql_mode = 'STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';";
    } | mariadb -u "$db_user" -p"$db_password" "$db_name"; then
        log_success "Database import completed successfully!"
        
        # Restore production settings after successful import
        restore_production_settings
    else
        log_error "Database import failed!"
        # Restore production settings even on failure
        restore_production_settings
        return 1
    fi
}

cleanup_file() {
    local file_path="$1"
    
    log_info "Cleaning up downloaded file..."
    
    if rm -f "$file_path"; then
        log_success "Temporary file deleted: $file_path"
    else
        log_warning "Failed to delete temporary file: $file_path"
    fi
}

confirm_import() {
    local file="$1"
    
    echo
    log_warning "‚ö†Ô∏è  WARNING: This will import data into your MariaDB database!"
    log_warning "‚ö†Ô∏è  This operation may overwrite existing data!"
    echo
    echo "Selected file: $file"
    echo "Database: ${MYSQL_DATABASE:-myapp}"
    echo
    
    read -p "Are you sure you want to proceed? (yes/no): " confirmation
    
    case "$confirmation" in
        [Yy][Ee][Ss]|[Yy])
            log_info "Proceeding with import..."
            return 0
            ;;
        *)
            log_info "Import cancelled by user."
            return 1
            ;;
    esac
}

main() {
    # Handle help immediately
    for arg in "$@"; do
        if [[ "$arg" == "--help" || "$arg" == "-h" ]]; then
            show_help
            exit 0
        fi
    done
    
    # Parse command line arguments
    parse_arguments "$@"
    
    echo "üóÑÔ∏è  Database Backup Downloader & Importer"
    echo "========================================="
    
    if [[ "$PURGE_BINLOGS" == "true" ]]; then
        echo "üóëÔ∏è  Binary log cleanup: ENABLED"
    else
        echo "üìù Binary log cleanup: DISABLED (use --purge-binlogs to enable)"
    fi
    echo
    
    # Check all requirements
    if ! check_requirements; then
        log_error "Requirements check failed. Cannot proceed with import."
        return 1
    fi
    
    # Configure AWS
    configure_aws
    
    # Load database files from S3
    if ! load_database_files; then
        log_error "Failed to load database files from S3. Cannot proceed."
        return 1
    fi
    
    # Show menu and get user choice
    show_menu
    if ! get_user_choice; then
        log_error "No file selected or user cancelled. Exiting."
        return 1
    fi
    
    # Confirm the import operation
    if ! confirm_import "$SELECTED_FILE"; then
        log_error "Import not confirmed. Exiting."
        return 1
    fi
    
    # Download the selected file
    local downloaded_file="$TEMP_DIR/$SELECTED_FILE"
    
    # Run download function
    if ! download_file "$SELECTED_FILE"; then
        log_error "Failed to download file: $SELECTED_FILE"
        return 1
    fi
    
    # Verify the file was downloaded
    if [[ ! -f "$downloaded_file" ]]; then
        log_error "Downloaded file not found: $downloaded_file"
        return 1
    fi
    
    # Import the database
    if ! import_database "$downloaded_file"; then
        log_error "Database import failed for file: $downloaded_file"
        # Still try to clean up
        cleanup_file "$downloaded_file"
        return 1
    fi
    
    # Clean up
    cleanup_file "$downloaded_file"
    
    echo
    log_success "‚úÖ Database import process completed successfully!"
    echo
}

# Handle script interruption
trap 'log_error "Script interrupted. Cleaning up..."; exit 1' INT TERM

# Run main function and return its exit code
if main "$@"; then
    exit 0
else
    exit 1
fi
